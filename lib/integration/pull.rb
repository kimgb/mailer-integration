class Mailer::Integration::Pull < Mailer::Integration
  attr_reader :gibbon

  def initialize(path)
    @gibbon = Gibbon::Request.new

    if path.directory?
      @base_dir = path
    else
      raise ArgumentError, "#{path} is not a directory"
    end

    # Some dastardly metaprogramming to set up some Sequel models from a user-supplied config file. Within the Pull class, we can use ::Campaign, ::Subscriber, and ::Junction when needed.
    ["campaign", "subscriber", "junction"].each do |m|
      # 'Dataset' in this case means table name - as defined in your config file.
      dataset = ::PULL_CONFIG[m.to_sym][:name].downcase.to_sym

      # Define a new Sequel::Model that looks up the table/dataset determined above.
      klass = Class.new(Sequel::Model(dataset)) do
        # The config file defines the assocations e.g. many-to-one between tables.
        ::PULL_CONFIG[m.to_sym][:associations].each do |assoc|
          # If your tables don't conform to Sequel conventions, you'll need to pass in some opts to get the foreign keys etc. right.
          method(assoc[:type]).call(assoc[:model], assoc[:opts])
        end
      end
      # a few alternatives here, depending on namespace preferences. because namespacing has been killing me lately, I'm putting them up high, but this should probably be reconsidered when time permits.
      Object.const_set(m.capitalize, klass)
    end
  end

  def notifier
    if ::PULL_CONFIG[:slack_webhook]
      @notifier ||= Slack::Notifier.new ::PULL_CONFIG[:slack_webhook], channel: "#comms"
    else
      false
    end
  end

  def run!
    logger.info "BEGINNING SYNC FROM MAILER"

    @this_run = Time.now
    # previous month, i.e., around 30 days ago. have to make a call here about where to cut the long tail off. 30 days is short, but definitely long enough to not miss out on a lot.
    # FIXME - overload read_runtime?
    filter = (Date.parse(read_runtime) << 1).strftime("%F")

    begin
      sync_campaigns(filter: filter)

      logger.info "Finished, saving this run as complete"
      save_runtime(@this_run)
    rescue Exception => err
      logger.error "#{err.class}: #{err.message}"
      logger.error err.backtrace
    ensure
      logger.close
    end
  end

  # Get campaigns, starting with the oldest. For each campaign, call sync_campaign.
  def sync_campaigns(opts={})
    offset = opts[:offset] ||= 0

    logger.info "REQUESTING FROM OFFSET #{offset}"
    response = list_campaigns(offset, opts[:filter])

    response.slice(*(0..9).map(&:to_s)).values.each do |campaign|
      sync_campaign(campaign)
    end

    unless response.body["total_items"] < (offset + 10) # 10 per request by default on campaigns
      sync_campaigns(offset: offset + 10, filter: opts[:filter])
    end
  end

  # Get the message(s), in plaintext. [Multiple messages in case of split test
  # campaigns.] Store the first as a Campaign.
  # - Get the recipients, in reverse order of priority: Unopened -> Opened ->
  #   Clicked -> Forwarded. Unsubscribe overrides all and cannot be overridden.
  #   Bounce overrides all but unsub, but is always overridden.
  # WARNING: Getting the recipient lists can be a hefty request! I've tested up
  # to ~8_300 records retrieved (>100_000 lines returned) without breaking.
  def sync_campaign(campaign)
    logger.info "STARTING SYNC FOR CAMPAIGN #{campaign["name"].upcase}"
    # Messages can be found at campaign["messages"]; an array. I'm not aware of any cases of messages arrays of size > 1, but should be prepared to handle it just in case. More than one message may occur in the case of a split test campaign. Since reports are generated by reference to the campaign, creating more than one row in the db per campaign will cause headaches.
    messages = campaign["messages"]

    if (message = messages.first)
      if notifier
        open_rate = Float(campaign["uniqueopens"]) / Float(campaign["send_amt"])
        open_rate = open_rate.nan? ? 0 : open_rate

        click_rate = Float(campaign["uniquelinkclicks"]) / Float(campaign["uniqueopens"])
        click_rate = click_rate.nan? ? 0 : click_rate

        notifier.ping "'#{message["subject"]}' - sent to #{campaign["send_amt"]}, with #{campaign["uniqueopens"]} unique opens (#{"%.2f%" % (open_rate * 100)} open rate), and #{campaign["uniquelinkclicks"]} unique link clicks (#{"%.2f%" % (click_rate * 100)} click rate)."
      end

      # store message to the database.
      @campaign = ::Campaign.find(CAMPAIGN[:campaign_id] => campaign["id"]) ||
        ::Campaign.new({
          CAMPAIGN[:campaign_id] => campaign["id"],
          CAMPAIGN[:message_subject] => message["subject"],
          CAMPAIGN[:message_text] => message["text"].gsub(/(?<!\r)\n/, "\r\n"),
          CAMPAIGN[:message_cdate] => message["cdate"],
          CAMPAIGN[:message_mdate] => message["mdate"]
        }.merge(CAMPAIGN[:static_cols] || {}))

      if @campaign.save
        # get the various campaign reports:
        # unopens would come first, but haven't been able to get them working

        # opens - paginated! recursive method, as with #sync_campaigns
        sync_campaign_opens(campaign["id"], @campaign)

        # clicks and forwards - need messageid as well as campaignid - need to iterate messages.
        sync_campaign_clicks_and_forwards(campaign["id"], messages, @campaign)

        # Bounces and unsubscribes - most straightforward of the lot
        sync_campaign_bounces(campaign["id"], @campaign)
        sync_campaign_unsubscribes(campaign["id"], @campaign)
      end
    else
      logger.info "No message found for campaign, skipping"
    end
  end

  # Snowballs a list of emails to be batch updated all at once, using recursion.
  # Once it runs out of pages, updates all such emails with the "opened" status.
  def sync_campaign_opens(id, campaign, emails=[], page=1)
    response = parse_request(list_opens(id, page))

    if response["result_code"].zero?
      emails.each_slice(::APP_CONFIG[:subset]) { |s| sync_contacts(s, "opened", campaign) }
    else
      logger.debug "Getting campaign opens, page #{page}"
      new_emails = response.except(*metadata).values.map { |c| c["email"] }
      sync_campaign_opens(id, campaign, emails + new_emails, page + 1)
    end
  end

  def sync_campaign_clicks_and_forwards(id, messages, campaign)
    logger.debug "Getting campaign clicks (and, one day, forwards)"
    messages.each do |msg|
      clicks = parse_request(list_clicks(id, msg["id"]))
      sync_campaign_clicks(clicks, campaign) unless clicks["result_code"].zero?
    end
  end

  def sync_campaign_clicks(clicks, campaign)
    links = clicks.except(*metadata).values.map { |link| link["info"] }.flatten
    emails = links.map { |click| click["email"] }

    emails.each_slice(::APP_CONFIG[:subset]) { |s| sync_contacts(s, "clicked", campaign) }
  end

  # can't build this one out until I find somewhere it's happened (in reality, until someone asks for it)
  def _sync_campaign_forwards(forwards, campaign)
  end

  def sync_campaign_bounces(id, campaign)
    logger.info "Getting campaign bounces"
    response = parse_request(list_bounces(id))

    unless response["result_code"].zero?
      emails = response.except(*metadata).values.map { |c| c["email"] }
      emails.each_slice(::APP_CONFIG[:subset]) { |s| sync_contacts(s, "bouncing", campaign) }
    end
  end

  def sync_campaign_unsubscribes(id, campaign)
    logger.info "Getting campaign unsubscribes"
    response = parse_request(list_unsubscribes(id))

    unless response["result_code"].zero?
      emails = response.except(*metadata).values.map { |c| c["email"] }
      emails.each_slice(::APP_CONFIG[:subset]) { |s| sync_contacts(s, "unsubscribed", campaign) }
    end
  end

  def metadata
    @metadata ||= ["result_code", "result_message", "result_output"]
  end

  def timer
    elapsed = Time.now - @timestamp
    @timestamp = Time.now

    elapsed
  end

  # Watch count vs. size. Can't call size on Sequel datasets. (Alias it?)
  # has grown a little out of hand. but it remains fairly DRY.
  def sync_contacts(emails, health, campaign)
    logger.info "Performing batch sync with #{emails.size} emails, starting at " +
      "#{@timestamp = Time.now}"

    # subscribers = ::Subscriber.where(SUBSCRIBER[:email] => emails)
    # sec_subscribers = ::Subscriber.where(SUBSCRIBER[:secondary_email] => emails)
    junction = ::Junction.where(campaign: campaign)
    logger.debug "Queries composed in #{timer}"

    # For each pair of relevant columns, look up and update subscribers who match an email.
    [{ email: :email, health: :health }, { email: :secondary_email, health: :secondary_health }].each do |e|
      subscribers = ::Subscriber.where(SUBSCRIBER[e[:email]] => emails)

      # Update junction records and subscribers, unless they're unsubscribed, with the latest health.
      subscriber_updates = subscribers.exclude(SUBSCRIBER[e[:health]] => ["unsubscribed", health]).update(SUBSCRIBER[e[:health]] => health)
      logger.info "Updated email health on #{subscriber_updates} Subscribers in #{timer}"

      # Update receipt status on the junction records by inner joining to subscribers
      junction_updates = junction.join(subscribers, JUNCTION[:subscriber_key] => SUBSCRIBER[:key]).exclude(JUNCTION[:receipt] => health).update(JUNCTION[:receipt] => health)
      logger.info "Updated receipt status on #{junction_updates} junction rows in #{timer}"

      # Create joins thru the junction table, where there are none yet. Note Sequel's double-underscore convention - shorthand for specifying explicitly the table and column to avoid ambiguity errors. Also note Sequel's t# convention - joined tables are aliased as t1, t2, etc.
      inserts = subscribers.left_join(junction, SUBSCRIBER[:key] => JUNCTION[:subscriber_key]).where("t1__#{JUNCTION[:campaign_key]}".to_sym => nil).select("#{JUNCTION[:subscriber]}__#{SUBSCRIBER[:key]}".to_sym).distinct.map { |c| [c[SUBSCRIBER[:key]], campaign[CAMPAIGN[:key]], *(JUNCTION[:static_cols] || {}).values, health] }
      logger.debug "Composed insert in #{timer}, now executing"

      # And insert them in a batch
      ::Junction.import([JUNCTION[:subscriber_key], JUNCTION[:campaign_key], *(JUNCTION[:static_cols] || {}).keys, JUNCTION[:receipt]], inserts)
      logger.info "Inserted #{inserts.size} new rows to the ContactRec table in #{timer}"
    end
  end

  private
  def parse_request(request_object)
    JSON.parse(http_root.request(request_object).body)
  end

  ## Methods below are for instantiating request objects to the mailer API.
  def list_campaigns(offset, since = nil)
    params = {
      list_id: ::PULL_CONFIG[:list_id], status: "sent", offset: offset,
      sort_field: "send_time", sort_dir: "ASC"
    }

    if since
      params[:since_send_time] = since
    end
    # returning 2015-12-14 - what the fuck. glad I went with the above version, but slightly in awe that I even came up with something so dense. -kbuckley
    # was gunning for impenetrability with this version:
    #params.[]=(*since ? [:filters,{ldate_since_datetime: since}] : [:ids,"all"])

    gibbon.campaigns.retrieve(params: params)
  end

  # So. These campaign report lists are super inconsistent. Some paginated (and sorted), some not. Some need message_id as well as campaign_id, some don't. Can't even get unopens working - from the API explorer, let alone in my own program! And they all return different data, arranged in different ways. Abstraction is difficult under the circumstances.

  # can't get this one working.
  def _list_unopens(campaign_id, message_id)
    query_params = queryise(api_key: ::APP_CONFIG[:api_key], api_output: "json",
      api_action: "campaign_report_unopen_list", campaignid: campaign_id,
      messageid: message_id)
    Net::HTTP::Get.new("/admin/api.php?#{query_params}")
  end

  # This is the only one with pagination and sorting
  def list_opens(campaign_id, page)
    query_params = queryise(api_key: ::APP_CONFIG[:api_key], api_output: "json",
      api_action: "campaign_report_open_list", campaignid: campaign_id,
      sort: "tstamp", sort_direction: "ASC", page: page)
    Net::HTTP::Get.new("/admin/api.php?#{query_params}")
  end

  # This one has the emails grouped by the link they clicked. Same email can
  # be repeated across different links.
  def list_clicks(campaign_id, message_id)
    query_params = queryise(api_key: ::APP_CONFIG[:api_key], api_output: "json",
      api_action: "campaign_report_link_list", campaignid: campaign_id,
      messageid: message_id)
    Net::HTTP::Get.new("/admin/api.php?#{query_params}")
  end

  # Although this one hasn't returning anything yet, I'm inclined to think it's
  # working. Forwarding is just very rare. Keep testing on bigger campaigns.
  def list_forwards(campaign_id, message_id)
    query_params = queryise(api_key: ::APP_CONFIG[:api_key], api_output: "json",
      api_action: "campaign_report_forward_list", campaignid: campaign_id,
      messageid: message_id)
    Net::HTTP::Get.new("/admin/api.php?#{query_params}")
  end

  # standard, keyed on incrementing id with some extra bounce info
  def list_bounces(campaign_id)
    query_params = queryise(api_key: ::APP_CONFIG[:api_key], api_output: "json",
      api_action: "campaign_report_bounce_list", campaignid: campaign_id)
    Net::HTTP::Get.new("/admin/api.php?#{query_params}")
  end

  # standard, keyed on incrementing id with some unsub info
  def list_unsubscribes(campaign_id)
    query_params = queryise(api_key: ::APP_CONFIG[:api_key], api_output: "json",
      api_action: "campaign_report_unsubscription_list",
      campaignid: campaign_id)
    Net::HTTP::Get.new("/admin/api.php?#{query_params}")
  end
end
